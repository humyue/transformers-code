{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本分类实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24461823bfa147ccbac03f02ed58ffbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f86f8ba8dc42ff851be6e89ea7c639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 7765\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"./ChnSentiCorp_htl_all.csv\", split=\"train\")\n",
    "dataset = dataset.filter(lambda x: x[\"review\"] is not None)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3 创建DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 6988\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 777\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = dataset.train_test_split(test_size=0.1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6988, 777)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets[\"train\"]),len(datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'review': '其实这个价格相对这个状况的酒店已经不便宜了（360元），这个季节，这个城市乱七八糟的。但是相对这个（4星）级别的，已经算是还可以了'} {'label': 1, 'review': '作为喜达屋的金卡会员,喜来登就是品质的保证,所以好的不多说了,鸡蛋里挑骨头说点需要改进的吧.1.房门自动关不紧,要拉一下,有点危险.2.前两次入住一早会把帐单从门缝里塞进来,感觉很细致,这次却没有,可能是入住人太多了忙不过来.3.中午吃的自助餐,海蟹肯定不太新鲜,要重视啊!4.最不爽的是：这一年价钱涨得太快了！另外，这次入住的是高级大床房，感觉反而没有舒适大床房好。个人觉得喜达屋旗下的酒店各有千秋，象艾美是奢华和大气，豪达喜来登是复古和庄重，福朋的特色则应该是精巧雅致，房间一大反而失去了那种紧凑感。（所以楼下有人评福朋的房间小应该就是指的舒适房，但岂不知这才是她的特色。）还有携程的信息可能有点问题，舒适大床房中的大床可能是queensize的，高级大床房的大床可能是kingsize，两者不仅是房间面积的不同，有机会应该核证一下。'}\n",
      "{'label': 0, 'review': '房间环境很差,没有窗户,而且入住时抽水马桶是坏的,打了两次电话给服务人员才来修理并且没有完全修好,提出换同等房间被拒绝.希望提高住宿环境和硬件质量.'} {'label': 1, 'review': '非常满意，服务贴心到位，还主动把携程的预定门票换成门票，我们可以不排队。房间干净整洁，还很漂亮，很赞的'}\n",
      "{'label': 1, 'review': '1、总体感觉有些旧，地毯上污迹比较多。2、服务等还可以。3、房间总带有一些烟味，不喜欢。'} {'label': 1, 'review': '果然如同其他会员们所说的：位置优越，但不太好找。让我体会了一把京城小胡同的韵味。周围还是在拆迁。酒店感觉较温馨，要是能够在每个房间里都放上吹风机就好了。还是值得向朋友们推荐的。'}\n",
      "{'label': 1, 'review': '酒店的设施是很不错的，房间大，设施新，床和被子都是很新的。洗手间也是干、湿分离。服务员态度也是很好，出入都会打招呼。就是餐厅大堂中，适合2－3人吃饭的桌子太少了。还有就是房价在衢州应该属于偏高了。'} {'label': 0, 'review': 'NND!才迟半个小时退房就收我半天的房费！而且也不提前打电话来通知一下！退房时掉了东西在房里他们也不会帮你找的！房间臭死了！离外滩远得要死打的要３０块！以后也不会住这个烂酒店了！ＭＤ,气！'}\n",
      "{'label': 1, 'review': '整体印象非常好，只是卫生间隔音太差，楼上洗澡发出的哗哗的水声恍若就在眼前。不过卫生间够大，小沙发也很舒服，服务也不错，还是会来住的！补充点评2008年5月18日：如果和如家快捷店去对比，那这里简直就是太超值了，洗澡那比如家强太多了，早餐不但免费，品种和质量比如家好多了，结束的时间也晚（10：00）。走廊宽敞明亮，房间又大，以前我还评价隔音不好，在青岛如家住完对比过后才明白，其实这里隔音效果比如家强多了。坐在这里前台办手续（前台服务员还多），既快又舒适，在如家你得站着七、八分钟办完就不错（前台最多两个服务员）。'} {'label': 1, 'review': '早餐食物相当一般，跟其服务水准不相符，希望进一步改进。'}\n",
      "{'label': 0, 'review': '房间冷的受不了，让服务员那条被子足足半小时，电话打了好几个，那床比木板还硬洗澡的水没有热气，这样的就店实在差劲，就是免费我也不会再住了'} {'label': 0, 'review': '在预定时间到前台进行登记时，服务人员告诉我们：XC没有进行预定，后来通过多次电话......，另外，预定价格和门市价格只有4元价差......'}\n",
      "{'label': 1, 'review': '不能住靠马路的房间,太吵,一晚上没睡好.旁边有个大超市,吃饭方便'} {'label': 1, 'review': '酒店很不错，硬件很好。自助餐厅早、中、晚都有很方便。性价比很高！'}\n",
      "{'label': 1, 'review': '只可惜硬件设施不完备，加上路边的噪音，给本来应是很完美的酒店留下了一点遗憾，希望如家的加盟店更上一层楼。'} {'label': 1, 'review': '住的时候是4月底，800元一天，第一，大堂气派，第二，房间干净。但，第一，大堂太多蚊子，吃饭时咬了很多包。第二，早上吃早餐，食物上有飞虫。第三，房间内有个夜灯无论如何无法关上，第二天服务员只好拔掉插头，我的一个同事住在别的房间也如此。第四，洗衣，价格贵，但，裤子和衬衣，都是熨烫出三条缝，很尴尬。实在不值800元的五星级。宾馆反馈2007年7月9日：首先代表东莞富盈酒店管理公司感谢下榻我店及给予点评的各位嘉宾,对于各位嘉宾点评到酒店的不足之处,管理公司的领导非常重视并立即做出了相应的更改和改善,对酒店的认同和褒奖,我们将会继续努力和发扬,也会一直关注各位嘉宾的点评,同时望各届人士能继续给予宝贵的意见,我们将竭尽全力做好各项服务工作.'}\n",
      "{'label': 1, 'review': '在呼市出差，入住金辉酒店。酒店的房间、设施等相关条件，在300元左右的房价中算不错的。服务员态度很好。估计因为是工行下属的关系吧，酒店里没有那些乱七八糟的人，也没有骚扰电话。最满意的是酒店的地理位置极佳，餐饮、购物、银行、景点全离得很近。下次来呼市还会选择这里。补充点评2008年7月14日：金辉酒店自己的自助餐是38元一位，菜品冷+热+主食大概在20种左右，感觉与38元的价格比较全理。酒店2楼餐厅也可以点菜，如果您是2位以上用餐，就选择点菜吧，菜价不算贵，菜量那是相当的大。服气了。'} {'label': 0, 'review': '一个充满铜臭的酒店。吸氧，每小时要收90元。办理入住，直接扣除2天房费，并声称若不足2天，不退款。热水龙头出水奇慢。相关问题，前台一问三不知。'}\n",
      "{'label': 1, 'review': '所在地应该是徐汇比较好的地段，很幽静，旁边就是爱菊小学，出门往右走到路口有超市和水果店，好像还有家西餐厅或是酒吧，但是其他吃饭的地方就不大好找了。酒店出门往左应该可以走到地铁衡山路站，哪个出口就不晓得了。酒店的环境很好，音乐喷泉，青翠草坪，感觉很不错。。。就是房间太小，甚至比香港的酒店还小，另外送餐的种类太少，早餐一般，没体现出上海的特色。。服务还是不错的，但是某些服务员较冷淡，算啦，反正你们的脸色也不是电视机屏幕，不用一直看的'} {'label': 1, 'review': '房间很干净，酒店的服务质量相比其它酒店更有特色，对前台人员印象很深，尤其是客服人员小秦，让人如沐春风。在此向各位会员推荐木棉花酒店。宾馆反馈2008年8月15日：非常感谢你的推荐，相信有了你的支持和认可我们的工作会做的更好！！！！期待您的下次光临！！'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(datasets[\"train\"][i],datasets[\"test\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5 创建Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b561fd616334263b27a0514b7cdfeae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6988 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67344b1d6ca4409d8b0031727b42d2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/777 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 6988\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 777\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/rbt3\")\n",
    "\n",
    "def process_function(examples):\n",
    "    tokenized_examples = tokenizer(examples[\"review\"], max_length=128, truncation=True)\n",
    "    tokenized_examples[\"labels\"] = examples[\"label\"]\n",
    "    return tokenized_examples\n",
    "\n",
    "tokenized_datasets = datasets.map(process_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "trainset, validset = tokenized_datasets[\"train\"], tokenized_datasets[\"test\"]\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "validloader = DataLoader(validset, batch_size=64, shuffle=False, collate_fn=DataCollatorWithPadding(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 6821,  702,  ...,    0,    0,    0],\n",
       "        [ 101,  679, 7231,  ...,  671, 4275,  102],\n",
       "        [ 101,  683, 7305,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101,  702,  782,  ...,    0,    0,    0],\n",
       "        [ 101,  126, 3299,  ...,  119, 6844,  102],\n",
       "        [ 101,  817, 3419,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 1, 0])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(trainloader))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step6 创建模型及优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/rbt3\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step7 训练与验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd5a693c8554efb9a4899062404d08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8980e70ff54f62ac084347299dc0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680cb190a44143c79ceee7d54213e479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ae3653716a4c3ba8e8ee38f2c8bcde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "clf_metrics = evaluate.combine([\"accuracy\",\"f1\",\"recall\",\"precision\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for batch in validloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            output = model(**batch)\n",
    "            pred = torch.argmax(output.logits, dim=-1)\n",
    "            clf_metrics.add_batch(predictions=pred.long(),references=batch[\"labels\"].long())\n",
    "            \n",
    "    return clf_metrics.compute()\n",
    "\n",
    "\n",
    "def train(epoch=3, log_step=100):\n",
    "    global_step = 0\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        for batch in trainloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.cuda() for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            output = model(**batch)\n",
    "            output.loss.backward()\n",
    "            optimizer.step()\n",
    "            if global_step % log_step == 0:\n",
    "                print(f\"ep: {ep}, global_step: {global_step}, loss: {output.loss.item()}\")\n",
    "            global_step += 1\n",
    "        clf = evaluate()\n",
    "        print(f\"ep: {ep}, {clf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, global_step: 0, loss: 0.29142141342163086\n",
      "ep: 0, global_step: 100, loss: 0.16152766346931458\n",
      "ep: 0, global_step: 200, loss: 0.21962319314479828\n",
      "ep: 0, {'accuracy': 0.8751608751608752, 'f1': 0.9066410009624639, 'recall': 0.8836772983114447, 'precision': 0.9308300395256917}\n",
      "ep: 1, global_step: 300, loss: 0.13791930675506592\n",
      "ep: 1, global_step: 400, loss: 0.34164679050445557\n",
      "ep: 1, {'accuracy': 0.879021879021879, 'f1': 0.9113207547169812, 'recall': 0.9061913696060038, 'precision': 0.9165085388994307}\n",
      "ep: 2, global_step: 500, loss: 0.2195776402950287\n",
      "ep: 2, global_step: 600, loss: 0.2111939787864685\n",
      "ep: 2, {'accuracy': 0.8751608751608752, 'f1': 0.9110907424381302, 'recall': 0.9324577861163227, 'precision': 0.8906810035842294}\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step8 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入：我觉得这家酒店不错，饭很好吃！\n",
      "模型预测结果:好评！\n",
      "Score:0.9975985884666443\n"
     ]
    }
   ],
   "source": [
    "sen = \"我觉得这家酒店不错，饭很好吃！\"\n",
    "id2_label = {0: \"差评！\", 1: \"好评！\"}\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    inputs = tokenizer(sen, return_tensors=\"pt\")\n",
    "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    logits = model(**inputs).logits\n",
    "    pred = torch.argmax(logits, dim=-1)\n",
    "    score = torch.softmax(logits, dim=-1)\n",
    "    print(f\"输入：{sen}\\n模型预测结果:{id2_label.get(pred.item())}\\nScore:{score[0][pred.item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model.config.id2label = id2_label\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '好评！', 'score': 0.9975985884666443}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '差评！', 'score': 0.8519829511642456}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"污渍不行\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
