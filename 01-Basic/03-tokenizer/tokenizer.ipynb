{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer基本使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大梦想\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 加载与保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从huggingface加载，输入模型名称，即可加载对应的分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./roberta_tokenizer\\\\tokenizer_config.json',\n",
       " './roberta_tokenizer\\\\special_tokens_map.json',\n",
       " './roberta_tokenizer\\\\vocab.txt',\n",
       " './roberta_tokenizer\\\\added_tokens.json',\n",
       " './roberta_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer保存到本地\n",
    "tokenizer.save_pretrained(\"./roberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从本地加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./roberta_tokenizer\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 句子分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sen)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 查看词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##朕': 16362,\n",
       " '倍': 945,\n",
       " '翩': 5431,\n",
       " '膏': 5601,\n",
       " '63': 8381,\n",
       " '##褫': 19250,\n",
       " '##ola': 12653,\n",
       " '##暫': 16328,\n",
       " '剐': 1186,\n",
       " 'docomo': 11358,\n",
       " '##堆': 14888,\n",
       " '缺': 5375,\n",
       " 'edward': 12259,\n",
       " '##line': 8762,\n",
       " '塢': 1854,\n",
       " '缭': 5369,\n",
       " '叁': 1345,\n",
       " '撅': 3049,\n",
       " '##ology': 10277,\n",
       " 'alt': 9721,\n",
       " '##謬': 19402,\n",
       " '诱': 6430,\n",
       " '稿': 4943,\n",
       " '699': 10439,\n",
       " '##ague': 12998,\n",
       " '猝': 4340,\n",
       " '##碧': 17876,\n",
       " '##細': 18226,\n",
       " '##炙': 17204,\n",
       " '##迎': 19873,\n",
       " '斑': 3157,\n",
       " '婉': 2039,\n",
       " 'chicago': 10268,\n",
       " '##铲': 20268,\n",
       " '捏': 2934,\n",
       " 'wma': 9900,\n",
       " '宙': 2136,\n",
       " '##隠': 20455,\n",
       " '孫': 2113,\n",
       " '-': 118,\n",
       " '##雍': 20477,\n",
       " '##黠': 21011,\n",
       " '##蓓': 18960,\n",
       " '骸': 7760,\n",
       " '##圃': 14804,\n",
       " '##硐': 17853,\n",
       " '樾': 3575,\n",
       " '乩': 742,\n",
       " '｡': 8081,\n",
       " 'dog': 13030,\n",
       " '##钓': 20214,\n",
       " '孙': 2101,\n",
       " '##ity': 8863,\n",
       " '##被': 19215,\n",
       " '帆': 2359,\n",
       " 'instagram': 8780,\n",
       " '##姝': 15060,\n",
       " '商': 1555,\n",
       " '##岚': 15326,\n",
       " '##彭': 15567,\n",
       " '##world': 10120,\n",
       " '##澀': 17123,\n",
       " '繩': 5256,\n",
       " '##⑩': 13565,\n",
       " '規': 6211,\n",
       " '##ads': 12514,\n",
       " '##伎': 13882,\n",
       " '##暄': 16315,\n",
       " '##襁': 19254,\n",
       " '庹': 2436,\n",
       " '啼': 1582,\n",
       " '＜': 8040,\n",
       " '##不': 13736,\n",
       " '##限': 20418,\n",
       " '壁': 1880,\n",
       " '囍': 1719,\n",
       " '水': 3717,\n",
       " 'null': 12083,\n",
       " '##军': 14149,\n",
       " '##盈': 17716,\n",
       " '裤': 6175,\n",
       " '荀': 5767,\n",
       " '跪': 6661,\n",
       " '皆': 4639,\n",
       " '##×': 9569,\n",
       " '軽': 6731,\n",
       " '<T>': 105,\n",
       " '##拾': 15953,\n",
       " '##队': 20396,\n",
       " '[unused28]': 28,\n",
       " '##雀': 20468,\n",
       " '1911': 10041,\n",
       " '##嗑': 14679,\n",
       " '##床': 15471,\n",
       " '##狰': 17384,\n",
       " '##99': 8653,\n",
       " '贝': 6564,\n",
       " '艘': 5675,\n",
       " '1884': 13007,\n",
       " '嫚': 2071,\n",
       " 'v': 164,\n",
       " '##のお': 10217,\n",
       " '##密': 15223,\n",
       " '##←': 13018,\n",
       " '##护': 15901,\n",
       " '##饨': 20703,\n",
       " '甫': 4502,\n",
       " 'pinkoi': 11941,\n",
       " 'lady': 8822,\n",
       " '##柱': 16450,\n",
       " '噸': 1697,\n",
       " 'beta': 9861,\n",
       " '##瑙': 17501,\n",
       " '##丿': 13773,\n",
       " '##契': 15000,\n",
       " '裸': 6180,\n",
       " '##菩': 18892,\n",
       " '様': 3545,\n",
       " '##永': 16776,\n",
       " '##馍': 20727,\n",
       " '##ﾄ': 21114,\n",
       " '盖': 4667,\n",
       " '##冈': 14139,\n",
       " '##説': 19361,\n",
       " '恚': 2611,\n",
       " '##よって': 12957,\n",
       " '##鰓': 20870,\n",
       " '俳': 937,\n",
       " 'for': 8330,\n",
       " '##媳': 15117,\n",
       " '##茜': 18809,\n",
       " '稔': 4926,\n",
       " '##都': 20020,\n",
       " '吗': 1408,\n",
       " '##蛆': 19082,\n",
       " '麥': 7930,\n",
       " '##楹': 16573,\n",
       " '糟': 5136,\n",
       " '##疝': 17605,\n",
       " '##自': 18689,\n",
       " '瑜': 4447,\n",
       " '慧': 2716,\n",
       " '鲨': 7835,\n",
       " '聳': 5477,\n",
       " 'wx17house': 9973,\n",
       " '厮': 1340,\n",
       " '羲': 5414,\n",
       " '##仕': 13856,\n",
       " '娇': 2019,\n",
       " '∀': 376,\n",
       " 'ⓘ': 426,\n",
       " '憋': 2728,\n",
       " '瘸': 4613,\n",
       " 'road': 9772,\n",
       " 'butler': 11797,\n",
       " '##mix': 11091,\n",
       " '##呦': 14509,\n",
       " '嘩': 1666,\n",
       " '¼': 189,\n",
       " '402': 12040,\n",
       " '##嫣': 15130,\n",
       " '##绀': 18354,\n",
       " '擾': 3101,\n",
       " '浦': 3855,\n",
       " '##ß': 13361,\n",
       " '##嚥': 14767,\n",
       " '##遐': 19941,\n",
       " '油': 3779,\n",
       " '##gnet': 13308,\n",
       " '鹑': 7906,\n",
       " 'cake': 11683,\n",
       " '穆': 4946,\n",
       " '##弃': 15518,\n",
       " '噁': 1679,\n",
       " 'chen': 9798,\n",
       " '幸': 2401,\n",
       " '##歎': 16683,\n",
       " '异': 2460,\n",
       " '阑': 7332,\n",
       " '##97': 9410,\n",
       " '##拱': 15947,\n",
       " 'cheese': 11387,\n",
       " '簌': 5078,\n",
       " '##胱': 18594,\n",
       " '瑰': 4456,\n",
       " 'install': 12461,\n",
       " '79': 8428,\n",
       " '碩': 4820,\n",
       " '箕': 5049,\n",
       " '賄': 6535,\n",
       " '##page': 11817,\n",
       " '矣': 4760,\n",
       " '##ther': 10198,\n",
       " '##翘': 18483,\n",
       " '3d': 8219,\n",
       " '銷': 7077,\n",
       " '骅': 7735,\n",
       " '咆': 1467,\n",
       " '痕': 4575,\n",
       " '孕': 2097,\n",
       " '铅': 7192,\n",
       " '##漩': 17090,\n",
       " '└': 435,\n",
       " '##灯': 17185,\n",
       " '1939': 9459,\n",
       " '🔥': 8103,\n",
       " '片': 4275,\n",
       " '##縄': 18291,\n",
       " '013': 13034,\n",
       " '##ors': 10903,\n",
       " '咘': 1478,\n",
       " '##釁': 20079,\n",
       " '扑': 2800,\n",
       " '镗': 7260,\n",
       " '##test': 11574,\n",
       " '算': 5050,\n",
       " '##log': 11458,\n",
       " '##利': 14221,\n",
       " '##劏': 14268,\n",
       " '##⒉': 13572,\n",
       " '##滕': 17058,\n",
       " '##嫩': 15132,\n",
       " '胺': 5542,\n",
       " '17': 8126,\n",
       " '忘': 2563,\n",
       " 'q': 159,\n",
       " '嗅': 1618,\n",
       " '杷': 3349,\n",
       " '铤': 7204,\n",
       " 'man': 8791,\n",
       " '轮': 6762,\n",
       " '##lus': 11131,\n",
       " '271': 11206,\n",
       " '蜜': 6057,\n",
       " '颓': 7575,\n",
       " '##脾': 18626,\n",
       " '##詹': 19342,\n",
       " '##諗': 19374,\n",
       " '##鷺': 20935,\n",
       " '##¹': 13357,\n",
       " '符': 5016,\n",
       " '##ste': 11881,\n",
       " '##冕': 14146,\n",
       " '##桃': 16482,\n",
       " '猜': 4339,\n",
       " '##裴': 19236,\n",
       " '♥': 491,\n",
       " '帷': 2381,\n",
       " '穫': 4953,\n",
       " '辽': 6808,\n",
       " '魏': 7794,\n",
       " '##漢': 17088,\n",
       " '##孚': 15159,\n",
       " '攪': 3115,\n",
       " '鹌': 7904,\n",
       " '疆': 4538,\n",
       " '升': 1285,\n",
       " '騁': 7696,\n",
       " '##备': 14963,\n",
       " '佐': 858,\n",
       " '脳': 5566,\n",
       " '##溜': 17034,\n",
       " '皓': 4645,\n",
       " '##氙': 16758,\n",
       " '##去': 14400,\n",
       " '拖': 2870,\n",
       " '##绥': 18381,\n",
       " '##窟': 18031,\n",
       " '财': 6568,\n",
       " '##還': 19974,\n",
       " '哪': 1525,\n",
       " '陶': 7378,\n",
       " '##歉': 16681,\n",
       " '／': 8027,\n",
       " 'matt': 12042,\n",
       " '##排': 16018,\n",
       " '肿': 5514,\n",
       " '☕': 482,\n",
       " 'young': 9802,\n",
       " '剩': 1197,\n",
       " '##另': 14426,\n",
       " '##镂': 20308,\n",
       " '赏': 6605,\n",
       " '簦': 5081,\n",
       " '蹬': 6700,\n",
       " '##ght': 9725,\n",
       " '拚': 2874,\n",
       " '##zl': 12961,\n",
       " '##股': 18557,\n",
       " '惋': 2662,\n",
       " '悻': 2655,\n",
       " '磡': 4835,\n",
       " '陋': 7358,\n",
       " '驼': 7729,\n",
       " '1919': 9915,\n",
       " '澄': 4067,\n",
       " '续': 5330,\n",
       " '金': 7032,\n",
       " '##tc': 10149,\n",
       " '##um': 8545,\n",
       " '##捡': 15996,\n",
       " 'facebooktwitterpinterestgoogle': 11498,\n",
       " '勘': 1242,\n",
       " '##浇': 16899,\n",
       " '極': 3513,\n",
       " '团': 1730,\n",
       " '廚': 2446,\n",
       " '皮': 4649,\n",
       " 'zol': 12182,\n",
       " '06': 8116,\n",
       " '##严': 13755,\n",
       " '揶': 3002,\n",
       " '菖': 5829,\n",
       " '##bert': 10491,\n",
       " '##キンク': 11505,\n",
       " '##劇': 14263,\n",
       " '菏': 5827,\n",
       " '##筊': 18081,\n",
       " '98': 8327,\n",
       " 'plc': 10015,\n",
       " '蜴': 6062,\n",
       " '归': 2495,\n",
       " '倒': 948,\n",
       " '踪': 6679,\n",
       " 'pmi': 10127,\n",
       " '▲topjan': 10601,\n",
       " '腕': 5580,\n",
       " 'forest': 10889,\n",
       " '佑': 859,\n",
       " '##雨': 20490,\n",
       " '勃': 1234,\n",
       " 'hall': 11049,\n",
       " '##壢': 14948,\n",
       " '哒': 1515,\n",
       " '##霸': 20521,\n",
       " '匪': 1272,\n",
       " '和': 1469,\n",
       " '6a': 11692,\n",
       " '##傭': 14056,\n",
       " '嗯': 1638,\n",
       " '##嬤': 15142,\n",
       " '鸦': 7887,\n",
       " '##tb': 11126,\n",
       " '##态': 15635,\n",
       " '##温': 17003,\n",
       " '拼': 2894,\n",
       " 'discuz': 13197,\n",
       " '##酐': 20039,\n",
       " '##亂': 13805,\n",
       " '##あります': 10700,\n",
       " '温': 3946,\n",
       " '鑫': 7144,\n",
       " '獸': 4366,\n",
       " '慘': 2711,\n",
       " 'div': 11308,\n",
       " '##钞': 20220,\n",
       " '##麓': 20983,\n",
       " '煜': 4207,\n",
       " '粹': 5122,\n",
       " '##蚯': 19078,\n",
       " '话': 6413,\n",
       " '##涵': 16948,\n",
       " '##犬': 17362,\n",
       " '✿': 504,\n",
       " '##葉': 18921,\n",
       " '綦': 5201,\n",
       " '##尉': 15259,\n",
       " '##rant': 10834,\n",
       " 'se': 9342,\n",
       " '##∙': 13531,\n",
       " '##柚': 16441,\n",
       " '氢': 3705,\n",
       " '辟': 6792,\n",
       " '幄': 2387,\n",
       " '詣': 6274,\n",
       " '蹄': 6686,\n",
       " '##距': 19712,\n",
       " 'wwdc': 12848,\n",
       " '##嫖': 15126,\n",
       " '##掩': 16030,\n",
       " '##驛': 20769,\n",
       " '##hy': 9943,\n",
       " '犄': 4299,\n",
       " '⌒': 404,\n",
       " '12345': 9700,\n",
       " '據': 3087,\n",
       " 'price': 11597,\n",
       " 'redis': 12599,\n",
       " '##弓': 15526,\n",
       " '##浪': 16914,\n",
       " '摻': 3046,\n",
       " '##め': 10063,\n",
       " '50cm': 11721,\n",
       " '##霞': 20516,\n",
       " '##搶': 16081,\n",
       " '##cker': 10603,\n",
       " '##⑶': 13568,\n",
       " '莅': 5797,\n",
       " '##赦': 19677,\n",
       " '荥': 5785,\n",
       " '鳥': 7852,\n",
       " '海': 3862,\n",
       " '動': 1240,\n",
       " '嘻': 1677,\n",
       " '衲': 6140,\n",
       " '##黜': 21008,\n",
       " '##520': 13241,\n",
       " '##钏': 20212,\n",
       " '##穩': 18009,\n",
       " '##≧': 13548,\n",
       " '##积': 17973,\n",
       " '##麗': 20984,\n",
       " '##~': 13347,\n",
       " '哦': 1521,\n",
       " '##dar': 12354,\n",
       " '耗': 5450,\n",
       " '井': 759,\n",
       " '##駕': 20747,\n",
       " 'how': 9510,\n",
       " '##综': 18398,\n",
       " '##邨': 19988,\n",
       " '殒': 3656,\n",
       " '##氨': 16767,\n",
       " '##▫': 13608,\n",
       " '##谦': 19529,\n",
       " '##詬': 19335,\n",
       " '##酥': 20046,\n",
       " '##胛': 18582,\n",
       " '综': 5341,\n",
       " '##葦': 18927,\n",
       " '##向': 14460,\n",
       " '睐': 4712,\n",
       " '##盂': 17712,\n",
       " '穢': 4951,\n",
       " '勳': 1251,\n",
       " 'x86': 10652,\n",
       " '##eh': 12742,\n",
       " '##咱': 14550,\n",
       " '党': 1054,\n",
       " '族': 3184,\n",
       " '##涇': 16923,\n",
       " '##猖': 17392,\n",
       " '##肿': 18571,\n",
       " '##躊': 19768,\n",
       " '臼': 5639,\n",
       " '淑': 3902,\n",
       " '##嗤': 14690,\n",
       " '##臓': 18682,\n",
       " '##coin': 10160,\n",
       " '啖': 1561,\n",
       " 'financial': 12200,\n",
       " '##吉': 14452,\n",
       " '矍': 4754,\n",
       " '##娼': 15092,\n",
       " '##儘': 14086,\n",
       " '氹': 3720,\n",
       " '錐': 7088,\n",
       " '影': 2512,\n",
       " 'false': 11457,\n",
       " '##く': 8763,\n",
       " '##端': 18056,\n",
       " 'サ': 603,\n",
       " '咸': 1496,\n",
       " 'apple': 8350,\n",
       " '##®': 8646,\n",
       " '##貌': 19562,\n",
       " '1965': 9141,\n",
       " '##苻': 18799,\n",
       " 'ddd': 13234,\n",
       " '##嫂': 15121,\n",
       " 'cba': 10912,\n",
       " '##窥': 18033,\n",
       " '姊': 1992,\n",
       " '##yi': 11017,\n",
       " '##涣': 16938,\n",
       " '术': 3318,\n",
       " '##蟒': 19154,\n",
       " '513': 13310,\n",
       " '##伦': 13897,\n",
       " '漯': 4037,\n",
       " '##醋': 20062,\n",
       " 'ᵘ': 336,\n",
       " '刃': 1145,\n",
       " '##誡': 19354,\n",
       " 'をこ': 10074,\n",
       " '1916': 10772,\n",
       " 'tel': 11108,\n",
       " '399': 9612,\n",
       " 'cool': 11338,\n",
       " '##颐': 20630,\n",
       " '##蛐': 19087,\n",
       " '##ary': 9277,\n",
       " 'jimmy': 10317,\n",
       " '渝': 3939,\n",
       " '##tant': 12028,\n",
       " '##碣': 17875,\n",
       " 'save': 13069,\n",
       " '麝': 7928,\n",
       " 'v9': 11894,\n",
       " 'message': 12231,\n",
       " '伺': 848,\n",
       " '##励': 14282,\n",
       " '##入': 14114,\n",
       " '##刽': 14232,\n",
       " '庾': 2437,\n",
       " '##惠': 15726,\n",
       " '##滷': 17075,\n",
       " '获': 5815,\n",
       " '##烬': 17234,\n",
       " 'わせ': 11878,\n",
       " 'river': 10835,\n",
       " '##跹': 19723,\n",
       " '##跩': 19717,\n",
       " '焦': 4193,\n",
       " '辗': 6786,\n",
       " '1971': 9061,\n",
       " '##耨': 18511,\n",
       " 'music': 9057,\n",
       " 'f1': 9080,\n",
       " '##柩': 16447,\n",
       " '##et': 8418,\n",
       " '##婆': 15095,\n",
       " '裆': 6164,\n",
       " '##賈': 19594,\n",
       " '鉑': 7058,\n",
       " '雋': 7418,\n",
       " '##羹': 18473,\n",
       " '##辫': 19854,\n",
       " '扦': 2808,\n",
       " '##書': 16349,\n",
       " '啥': 1567,\n",
       " 'harry': 12296,\n",
       " '##畴': 17590,\n",
       " '挟': 2911,\n",
       " '霎': 7453,\n",
       " '##垚': 14857,\n",
       " '25000': 13251,\n",
       " '##夹': 14988,\n",
       " '##这': 19878,\n",
       " '##养': 14132,\n",
       " '栩': 3414,\n",
       " '濒': 4085,\n",
       " '謗': 6339,\n",
       " '个': 702,\n",
       " '殿': 3671,\n",
       " '笛': 5013,\n",
       " '紺': 5172,\n",
       " '线': 5296,\n",
       " 'ｋ': 8061,\n",
       " '睿': 4729,\n",
       " '栅': 3402,\n",
       " '梟': 3455,\n",
       " '渠': 3940,\n",
       " '锏': 7228,\n",
       " 'nb': 9254,\n",
       " '烫': 4176,\n",
       " 'well': 12010,\n",
       " '僧': 1014,\n",
       " '尕': 2210,\n",
       " '鹉': 7902,\n",
       " '##ome': 9893,\n",
       " '併': 882,\n",
       " '粤': 5113,\n",
       " '##os': 8470,\n",
       " '##sc': 10203,\n",
       " '★★★★': 11815,\n",
       " '##fw': 12851,\n",
       " '##审': 15201,\n",
       " '##懶': 15811,\n",
       " '##祇': 17913,\n",
       " '##辩': 19853,\n",
       " '##酩': 20047,\n",
       " '##陲': 20432,\n",
       " '坏': 1776,\n",
       " '直': 4684,\n",
       " '##鉑': 20115,\n",
       " '##鸵': 20950,\n",
       " '##乒': 13785,\n",
       " '##哑': 14571,\n",
       " '悪': 2646,\n",
       " '##搭': 16079,\n",
       " '##lution': 11843,\n",
       " '##稜': 17986,\n",
       " '##月': 16356,\n",
       " '##輕': 19795,\n",
       " '籍': 5093,\n",
       " '##肆': 18544,\n",
       " '##氰': 16772,\n",
       " '十': 1282,\n",
       " '##舞': 18716,\n",
       " '##29': 8887,\n",
       " '##多': 14971,\n",
       " '##atus': 12283,\n",
       " '##蒼': 18952,\n",
       " '##と': 8322,\n",
       " '##ook': 10176,\n",
       " '##xy': 11038,\n",
       " '##厉': 14383,\n",
       " 'よ': 577,\n",
       " '##泮': 16860,\n",
       " '①': 405,\n",
       " '亢': 768,\n",
       " 'banner': 13256,\n",
       " '##勢': 14305,\n",
       " '##谑': 19513,\n",
       " '##鳃': 20899,\n",
       " '##娅': 15074,\n",
       " '##漂': 17080,\n",
       " '##▬': 13609,\n",
       " '孝': 2105,\n",
       " '冒': 1088,\n",
       " '姨': 2007,\n",
       " '##婕': 15098,\n",
       " '謀': 6331,\n",
       " '3300': 11543,\n",
       " '82': 8460,\n",
       " '扱': 2818,\n",
       " '讧': 6373,\n",
       " '##佳': 13938,\n",
       " '炭': 4151,\n",
       " '缠': 5362,\n",
       " '饲': 7654,\n",
       " '耷': 5457,\n",
       " '266': 9674,\n",
       " '4200': 12395,\n",
       " '撮': 3065,\n",
       " '##alk': 11346,\n",
       " '蕭': 5941,\n",
       " 'ｌ': 8062,\n",
       " '##蒜': 18943,\n",
       " '##犁': 17355,\n",
       " '##隻': 20464,\n",
       " '950': 10468,\n",
       " '历': 1325,\n",
       " '1940': 9211,\n",
       " 'dec': 9333,\n",
       " '蒼': 5895,\n",
       " '##固': 14800,\n",
       " '##异': 15517,\n",
       " '##sa': 8606,\n",
       " '##紀': 18202,\n",
       " '##rp': 9980,\n",
       " '弁': 2459,\n",
       " 'gov': 9514,\n",
       " '##围': 14798,\n",
       " '##郦': 20011,\n",
       " '梢': 3456,\n",
       " '##既': 16245,\n",
       " 'gis': 12892,\n",
       " '42': 8239,\n",
       " '[unused82]': 82,\n",
       " '251': 10924,\n",
       " 'disney': 12299,\n",
       " '##0mm': 12483,\n",
       " 'scott': 10692,\n",
       " 'alan': 11648,\n",
       " '818': 12265,\n",
       " '##修': 13991,\n",
       " '##抡': 15899,\n",
       " '試': 6275,\n",
       " 'macd': 10851,\n",
       " '释': 7025,\n",
       " '1888': 10988,\n",
       " 'mozilla': 11579,\n",
       " '##〇': 13649,\n",
       " '点': 4157,\n",
       " '︿': 7995,\n",
       " '##蚓': 19070,\n",
       " '嗟': 1630,\n",
       " '蜚': 6056,\n",
       " '##瞅': 17788,\n",
       " '##fx': 13122,\n",
       " '##峇': 15337,\n",
       " '##喂': 14642,\n",
       " '玻': 4390,\n",
       " '蕁': 5930,\n",
       " '##哪': 14582,\n",
       " '傩': 997,\n",
       " '##渥': 17001,\n",
       " 'f3': 11651,\n",
       " '噔': 1683,\n",
       " '##蕩': 18996,\n",
       " 'mx': 11171,\n",
       " '##燿': 17311,\n",
       " '##钟': 20221,\n",
       " '闡': 7303,\n",
       " '##戴': 15842,\n",
       " '##81': 9313,\n",
       " 'mar': 9118,\n",
       " '13': 8124,\n",
       " '肯': 5507,\n",
       " '##own': 11751,\n",
       " '##下': 13735,\n",
       " '銅': 7067,\n",
       " '單': 1606,\n",
       " '淚': 3907,\n",
       " 'には': 8738,\n",
       " 'group': 9051,\n",
       " '鈴': 7051,\n",
       " '##茨': 18811,\n",
       " 'site': 11215,\n",
       " '52kb': 12811,\n",
       " '豈': 6488,\n",
       " '##ˈ': 13372,\n",
       " '纬': 5281,\n",
       " '舱': 5665,\n",
       " '出': 1139,\n",
       " '##交': 13826,\n",
       " 'wi': 8541,\n",
       " '##左': 15397,\n",
       " '##灑': 17177,\n",
       " '##谶': 19540,\n",
       " '##﹡': 21068,\n",
       " '##覓': 19269,\n",
       " '##app': 11259,\n",
       " '评': 6397,\n",
       " 'bug': 8761,\n",
       " '##key': 9938,\n",
       " 'balance': 11606,\n",
       " '50mm': 11774,\n",
       " '谩': 6475,\n",
       " '##rio': 12062,\n",
       " '##5s': 13201,\n",
       " '##ルハイト': 13304,\n",
       " '##捷': 16006,\n",
       " '##焰': 17252,\n",
       " '##80': 8538,\n",
       " '##覧': 19273,\n",
       " '介': 792,\n",
       " 'ebay': 8886,\n",
       " '##)': 13325,\n",
       " '左': 2340,\n",
       " '統': 5186,\n",
       " 'wind': 12302,\n",
       " '##气': 16755,\n",
       " '801': 12566,\n",
       " '痨': 4585,\n",
       " '##丁': 13729,\n",
       " '讷': 6386,\n",
       " 'ィ': 590,\n",
       " '##癖': 17676,\n",
       " 'ᅢ': 305,\n",
       " '赋': 6602,\n",
       " '锅': 7222,\n",
       " 'candy': 10653,\n",
       " '##殓': 16714,\n",
       " '拱': 2890,\n",
       " '##フト': 10868,\n",
       " '凿': 1142,\n",
       " '##倪': 14017,\n",
       " 'ⓒ': 424,\n",
       " '誨': 6302,\n",
       " '##蔭': 18980,\n",
       " '##薬': 19017,\n",
       " '馋': 7669,\n",
       " 'ي': 274,\n",
       " '##ⅳ': 13522,\n",
       " '熒': 4222,\n",
       " '蛭': 6036,\n",
       " 'ai': 8578,\n",
       " 'room': 10083,\n",
       " '##1000': 10529,\n",
       " '##倾': 14024,\n",
       " '##笹': 18076,\n",
       " '##绍': 18362,\n",
       " '##薰': 19019,\n",
       " '##貪': 19574,\n",
       " '▲topaug': 10558,\n",
       " '##浬': 16915,\n",
       " 'network': 10339,\n",
       " 'earth': 11409,\n",
       " '緩': 5227,\n",
       " '375': 11256,\n",
       " '##恚': 15668,\n",
       " '##tional': 11852,\n",
       " '鰓': 7813,\n",
       " 'chapter': 12350,\n",
       " '##任': 13875,\n",
       " '##动': 14277,\n",
       " '##˚': 13378,\n",
       " '擡': 3090,\n",
       " '##腐': 18633,\n",
       " 'ᅩ': 311,\n",
       " '##衩': 19192,\n",
       " 'kanye': 8598,\n",
       " '##疸': 17618,\n",
       " '躁': 6708,\n",
       " '焕': 4185,\n",
       " '##恤': 15671,\n",
       " '颱': 7593,\n",
       " '1983': 8715,\n",
       " '讚': 6367,\n",
       " '##佚': 13923,\n",
       " 'ァ': 588,\n",
       " '##ugh': 12667,\n",
       " '##蜢': 19116,\n",
       " '##堺': 14901,\n",
       " '煅': 4199,\n",
       " '┃': 430,\n",
       " '鬆': 7777,\n",
       " '##晖': 16296,\n",
       " '餮': 7632,\n",
       " '##bi': 9350,\n",
       " '185': 9560,\n",
       " 'node': 10320,\n",
       " '##㗎': 13727,\n",
       " '舺': 5671,\n",
       " 'ubuntuforumwikilinuxpastechat': 12076,\n",
       " '##削': 14238,\n",
       " '##谅': 19503,\n",
       " '##廢': 15507,\n",
       " '橄': 3576,\n",
       " '[unused96]': 96,\n",
       " '▍': 457,\n",
       " '结': 5310,\n",
       " '##bon': 12229,\n",
       " '##誥': 19357,\n",
       " 'el': 10245,\n",
       " 'code': 8700,\n",
       " '么': 720,\n",
       " '##淆': 16955,\n",
       " '##釜': 20092,\n",
       " '😎': 8105,\n",
       " '##錠': 20148,\n",
       " '荞': 5779,\n",
       " 'mind': 12573,\n",
       " 'º': 187,\n",
       " '##ⅲ': 13521,\n",
       " '##夥': 14976,\n",
       " '樑': 3558,\n",
       " '##盎': 17718,\n",
       " '拒': 2867,\n",
       " '襁': 6197,\n",
       " '習': 5424,\n",
       " '##壑': 14942,\n",
       " '##or': 8372,\n",
       " '##覺': 19278,\n",
       " '##012': 12037,\n",
       " '##顫': 20605,\n",
       " '恭': 2621,\n",
       " '##闕': 20356,\n",
       " '摄': 3029,\n",
       " '邺': 6942,\n",
       " '召': 1374,\n",
       " '媛': 2056,\n",
       " '溅': 3972,\n",
       " '##贾': 19650,\n",
       " '##办': 14272,\n",
       " '[unused41]': 41,\n",
       " '##嗽': 14701,\n",
       " '##輪': 19800,\n",
       " 'zara': 10464,\n",
       " '逞': 6861,\n",
       " '137': 9444,\n",
       " '##坞': 14840,\n",
       " '##缨': 18423,\n",
       " '##跪': 19718,\n",
       " '##進': 19925,\n",
       " '##khz': 12112,\n",
       " '##餚': 20684,\n",
       " '##嚎': 14760,\n",
       " 'lulu': 11970,\n",
       " '獭': 4361,\n",
       " 'tiger': 12088,\n",
       " '##3000': 13141,\n",
       " '080': 12365,\n",
       " '暫': 3271,\n",
       " '人': 782,\n",
       " '##湊': 17014,\n",
       " '##ل': 13435,\n",
       " '卿': 1321,\n",
       " 'ngo': 11770,\n",
       " '787': 13029,\n",
       " '类': 5102,\n",
       " '##邂': 19972,\n",
       " '四': 1724,\n",
       " '嚮': 1712,\n",
       " '##肠': 18556,\n",
       " '筍': 5026,\n",
       " '籃': 5091,\n",
       " '哨': 1523,\n",
       " '鱔': 7820,\n",
       " 'center': 9469,\n",
       " '##work': 12282,\n",
       " '##盤': 17733,\n",
       " '櫚': 3603,\n",
       " '##窿': 18041,\n",
       " '珈': 4394,\n",
       " '##個': 14000,\n",
       " '##捎': 15990,\n",
       " 'sb': 12359,\n",
       " '##窈': 18020,\n",
       " '牲': 4291,\n",
       " '##溝': 17035,\n",
       " '孤': 2109,\n",
       " '髦': 7771,\n",
       " '##sun': 11798,\n",
       " '臻': 5638,\n",
       " '##眸': 17761,\n",
       " '##踱': 19738,\n",
       " 'inside': 13157,\n",
       " '##se': 8417,\n",
       " '惟': 2668,\n",
       " '撫': 3062,\n",
       " '榻': 3536,\n",
       " '##ico': 10641,\n",
       " '360°': 11739,\n",
       " '##柳': 16451,\n",
       " '角': 6235,\n",
       " 'える': 10382,\n",
       " 'φ': 229,\n",
       " '##突': 18017,\n",
       " '##蔗': 18972,\n",
       " '淞': 3908,\n",
       " '##得': 15590,\n",
       " '祐': 4860,\n",
       " '111': 8932,\n",
       " '##崆': 15356,\n",
       " '##譬': 19414,\n",
       " '毁': 3673,\n",
       " '辖': 6785,\n",
       " '##莺': 18874,\n",
       " '乱': 744,\n",
       " '韦': 7504,\n",
       " '褻': 6196,\n",
       " 'september': 10024,\n",
       " '##ɡ': 13369,\n",
       " '##樽': 16631,\n",
       " 'published': 12821,\n",
       " 'cl': 12847,\n",
       " '##櫚': 16660,\n",
       " '诞': 6414,\n",
       " '湟': 3962,\n",
       " '##篷': 18133,\n",
       " '##痺': 17649,\n",
       " '##land': 8789,\n",
       " '蒞': 5887,\n",
       " 'eclipse': 11752,\n",
       " '略': 4526,\n",
       " 'real': 10153,\n",
       " '翦': 5430,\n",
       " '庆': 2412,\n",
       " 'dit': 13231,\n",
       " '##梗': 16510,\n",
       " '郜': 6949,\n",
       " '##b': 8204,\n",
       " '##痤': 17640,\n",
       " '##知': 17818,\n",
       " '撈': 3051,\n",
       " 's': 161,\n",
       " '1934': 9823,\n",
       " '闸': 7316,\n",
       " '##ten': 11598,\n",
       " '##繆': 18305,\n",
       " '##觐': 19290,\n",
       " '##詞': 19327,\n",
       " '##郝': 20007,\n",
       " '##韦': 20561,\n",
       " '##nne': 12866,\n",
       " '刎': 1151,\n",
       " '辜': 6790,\n",
       " '6': 127,\n",
       " '##˙': 13377,\n",
       " '皿': 4654,\n",
       " '決': 3748,\n",
       " '達': 6888,\n",
       " '捍': 2932,\n",
       " '##餌': 20679,\n",
       " '糾': 5144,\n",
       " '屆': 2234,\n",
       " '2l': 13191,\n",
       " '##査': 16454,\n",
       " '貌': 6505,\n",
       " '##痉': 17627,\n",
       " '##谟': 19524,\n",
       " '4d': 10571,\n",
       " '持': 2898,\n",
       " '##韵': 20567,\n",
       " '##〔': 13661,\n",
       " '##io': 8652,\n",
       " '##蜻': 19121,\n",
       " '##魏': 20851,\n",
       " '麵': 7934,\n",
       " '##均': 14829,\n",
       " '##懷': 15812,\n",
       " '卯': 1312,\n",
       " '##沧': 16828,\n",
       " '噱': 1694,\n",
       " '槿': 3554,\n",
       " '##魔': 20852,\n",
       " '樹': 3572,\n",
       " '昔': 3212,\n",
       " '谄': 6445,\n",
       " '##il': 8742,\n",
       " '##負': 19568,\n",
       " '1001': 11823,\n",
       " '##艰': 18737,\n",
       " '亲': 779,\n",
       " '诵': 6433,\n",
       " '##祷': 17933,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 索引转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将词序列转换为id序列\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['弱', '小', '的', '我', '也', '有', '大', '梦', '想']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为token序列\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'弱 小 的 我 也 有 大 梦 想'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将token序列转换为string\n",
    "str_sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "str_sen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 更便捷的实现方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 10252, 8221, 106, 102]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将字符串转换为id序列，又称之为编码\n",
    "# ids = tokenizer.encode(sen, add_special_tokens=False)\n",
    "ids = tokenizer.encode(sen, add_special_tokens=True)  # 默认为True\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 弱 小 的 我 也 有 大 dreaming! [SEP]'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将id序列转换为字符串，又称之为解码\n",
    "str_sen = tokenizer.decode(ids, skip_special_tokens=False)  # 默认为False\n",
    "str_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 填充与截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 102, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 填充\n",
    "ids = tokenizer.encode(sen, max_length=15,  padding=\"max_length\")\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 弱 小 的 我 也 有 大 梦 想 [SEP] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_sen = tokenizer.decode(ids, skip_special_tokens=False) # 默认为True\n",
    "str_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 102]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 截断\n",
    "ids = tokenizer.encode(sen, max_length=5, truncation=True)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 弱 小 的 [SEP]'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_sen = tokenizer.decode(ids, skip_special_tokens=False) # 默认为True\n",
    "str_sen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 其他输入部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 102, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(sen, padding=\"max_length\", max_length=15)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 102, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = [1 if idx != 0 else 0 for idx in ids]\n",
    "token_type_ids = [0] * len(ids)\n",
    "ids, attention_mask, token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 快速调用方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 102, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 102, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(sen, padding=\"max_length\", max_length=15)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 处理batch数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 3457, 2682, 102], [101, 3300, 3457, 2682, 6443, 6963, 749, 679, 6629, 102], [101, 6841, 6852, 3457, 2682, 4638, 2552, 8024, 3683, 3457, 2682, 3315, 6716, 8024, 3291, 1377, 6586, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sens = [\"弱小的我也有大梦想\",\n",
    "        \"有梦想谁都了不起\",\n",
    "        \"追逐梦想的心，比梦想本身，更可贵\"]\n",
    "res = tokenizer(sens)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 53 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(1000):\n",
    "    tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 10 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = tokenizer([sen] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./roberta_tokenizer', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast / Slow Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"弱小的我也有大Dreaming!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\")  # 默认是Fast，没有Fast则是Slow\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='uer/roberta-base-finetuned-dianping-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_tokenizer = AutoTokenizer.from_pretrained(\"uer/roberta-base-finetuned-dianping-chinese\", use_fast=False)\n",
    "slow_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 469 ms\n",
      "Wall time: 551 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    fast_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.47 s\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 单条循环处理\n",
    "for i in range(10000):\n",
    "    slow_tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 344 ms\n",
      "Wall time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = fast_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.27 s\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 处理batch数据\n",
    "res = slow_tokenizer([sen] * 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2483, 2207, 4638, 2769, 738, 3300, 1920, 10252, 8221, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 12), (12, 15), (15, 16), (0, 0)]}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = fast_tokenizer(sen, return_offsets_mapping=True)  # 多了offset_mapping参数\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mslow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 报错\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2945\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2943\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2944\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2945\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2947\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3053\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   3033\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3034\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3050\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3051\u001b[0m     )\n\u001b[0;32m   3052\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   3054\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   3055\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   3056\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3057\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3058\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   3059\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3060\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3061\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3062\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3063\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3064\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3065\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3066\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3067\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3068\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3069\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3070\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3071\u001b[0m         split_special_tokens\u001b[38;5;241m=\u001b[39msplit_special_tokens,\n\u001b[0;32m   3072\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3073\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3127\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3117\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3118\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3119\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3120\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3124\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3125\u001b[0m )\n\u001b[1;32m-> 3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[0;32m   3128\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   3129\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   3130\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3131\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3132\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3133\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3134\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3135\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3136\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3137\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3138\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3139\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3140\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3141\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3142\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3143\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3144\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3145\u001b[0m     split_special_tokens\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_special_tokens),\n\u001b[0;32m   3146\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3147\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\tokenization_utils.py:780\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    775\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    776\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    777\u001b[0m             )\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m--> 780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    781\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    782\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.PreTrainedTokenizerFast. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMore information on available tokenizers at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    786\u001b[0m     )\n\u001b[0;32m    788\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    789\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: return_offset_mapping is not available when using Python tokenizers. To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast. More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674"
     ]
    }
   ],
   "source": [
    "inputs = slow_tokenizer(sen, return_offsets_mapping=True)  # 报错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特殊Tokenizer的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkyworkTokenizer(name_or_path='Skywork/Skywork-13B-base', vocab_size=65519, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新版本的transformers（>4.34），加载 THUDM/chatglm 会报错，因此这里替换为了天宫的模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Skywork/Skywork-13B-base\", trust_remote_code=True)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)  # 旧版本\n",
    "tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('skywork_tokenizer\\\\tokenizer_config.json',\n",
       " 'skywork_tokenizer\\\\special_tokens_map.json',\n",
       " 'skywork_tokenizer\\\\tokenizer.model',\n",
       " 'skywork_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"skywork_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"skywork_tokenizer\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>弱小的我也有大Dreaming!'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 29871, 55215, 30210, 30672, 30953, 30417, 30257, 29928, 1633, 292, 29991], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
